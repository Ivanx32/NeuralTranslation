{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "from keras.models import Model;\n",
    "from keras.layers.embeddings import Embedding;\n",
    "from keras.models import Sequential,load_model;\n",
    "from keras.optimizers import rmsprop,adam,adagrad,SGD;\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau, CSVLogger;\n",
    "from keras.preprocessing.text import text_to_word_sequence,one_hot,Tokenizer;\n",
    "from keras.layers import Input,Dense,merge,Dropout,BatchNormalization,Activation,Conv1D;\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# setting current working directory\n",
    "\n",
    "WKDIR='/home/ivan/Dropbox/Projects/OldLang/code/ByteNet-Keras-master'\n",
    "data_path = '/home/ivan/Dropbox/Projects/OldLang/data/ancient/' + 'processed_v6_maxlen205.txt' #!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, data_path, N=150000):\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    anc_list = []\n",
    "    rus_list = []\n",
    "    for line in lines[: len(lines) - 1]:\n",
    "        anc, rus = line.split('\\t')\n",
    "        anc_list += [anc]\n",
    "        rus_list += [rus]\n",
    "    \n",
    "    French = rus_list\n",
    "    English = anc_list\n",
    "    English = [i + \"\\n\" for i in English];# add ending signal at the sequence end\n",
    "    while 1:\n",
    "        if len(English) % batch_size != 0:\n",
    "            del English[-1];\n",
    "            del French[-1];\n",
    "        else:\n",
    "            break;\n",
    "    return French,English;\n",
    "\n",
    "\n",
    "French,English = load_dataset(batch_size=50, data_path=data_path, N=1000) #!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15100, 15100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(French), len(English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['так начнем же повесть эту',\n",
       " 'после потопа трое сыновей ноя разделили землю сим хам иафет',\n",
       " 'на востоке же находятся киликия памфилия писидия мисия ликаония фригия камалия ликия кария лидия другая мисия троада эолида вифиния старая фригия',\n",
       " 'туда же относятся и острова некие сардиния крит кипр и река геона называемая нил',\n",
       " 'в иафетовой же части обитает русь чудь и всякие народы меря мурома весь мордва заволочьская чудь пермь печера ямь угра литва зимигола корсь летгола ливы']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "French[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vacabulary(French,English):\n",
    "    all_eng_words = [];\n",
    "    all_french_words = [];\n",
    "    for i in np.arange(0, len(French)):\n",
    "        all_eng_words.append(English[i]);\n",
    "        all_french_words.append(French[i]);\n",
    "    tokeng = Tokenizer(char_level=True);\n",
    "    tokeng.fit_on_texts(all_eng_words);\n",
    "    eng_index = tokeng.word_index;  # build character to index dictionary\n",
    "    index_eng = dict((eng_index[i], i) for i in eng_index);\n",
    "    tokita = Tokenizer(char_level=True);\n",
    "    tokita.fit_on_texts(all_french_words);\n",
    "    french_index = tokita.word_index;  # build character to index dictionary\n",
    "    index_french = dict((french_index[i], i) for i in french_index);\n",
    "    return (eng_index,french_index,index_eng,index_french);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert a batch of input sequences to tensors\n",
    "def generate_batch_data(English,French,eng_index,french_index,batch_size):\n",
    "    while 1:\n",
    "        all_labels=np.arange(0,len(French));np.random.shuffle(all_labels);\n",
    "        batch_labels=np.array_split(all_labels,int(len(French)*batch_size**-1));\n",
    "        for labels in batch_labels:\n",
    "            source_vec=np.zeros((batch_size,maxlen+1),dtype=np.uint16);\n",
    "            target0_vec=np.zeros((batch_size,maxlen),dtype=np.uint16);\n",
    "            target1_vec = np.zeros((batch_size, maxlen+1, len(eng_index)), dtype=np.uint16);\n",
    "            sampleweights=np.zeros((batch_size,maxlen+1),dtype=np.uint16);\n",
    "            for i,a in enumerate(labels):\n",
    "                for j1,ele1 in enumerate(French[a]):\n",
    "                    source_vec[i,j1]=french_index[ele1];\n",
    "                for j2,ele2 in enumerate(English[a][:-1]):\n",
    "                    target0_vec[i,j2]=eng_index[ele2];\n",
    "                for j3,ele3 in enumerate(English[a]):\n",
    "                    target1_vec[i,j3,eng_index[ele3]-1]=1;\n",
    "                    sampleweights[i,j3]=1;# mask the loss function\n",
    "            t0=np.zeros((batch_size,1,500),dtype=np.uint8);# beginning of target sequence\n",
    "            yield ([source_vec,target0_vec,t0],target1_vec,sampleweights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(french_index,eng_index,index_french,index_eng,English,French):\n",
    "    input_sequence = Input(shape=(maxlen + 1,));\n",
    "    input_tensor = Embedding(input_length=maxlen + 1, input_dim=len(french_index) + 1, output_dim=500)(input_sequence);\n",
    "    encoder1 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(input_tensor);\n",
    "    encoder1 = Activation(\"relu\")(encoder1);\n",
    "    encoder1 = Conv1D(filters=250, kernel_size=5, strides=1, padding=\"same\", dilation_rate=1)(encoder1);\n",
    "    encoder1 = BatchNormalization(axis=-1)(encoder1);\n",
    "    encoder1 = Activation(\"relu\")(encoder1);\n",
    "    encoder1 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder1);\n",
    "    input_tensor = merge([input_tensor, encoder1], mode=\"sum\");\n",
    "    encoder2 = BatchNormalization(axis=-1)(input_tensor);\n",
    "    encoder2 = Activation(\"relu\")(encoder2);\n",
    "    encoder2 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(input_tensor);\n",
    "    encoder2 = BatchNormalization(axis=-1)(encoder2);\n",
    "    encoder2 = Activation(\"relu\")(encoder2);\n",
    "    encoder2 = Conv1D(filters=250, kernel_size=5, strides=1, padding=\"same\", dilation_rate=2)(encoder2);\n",
    "    encoder2 = BatchNormalization(axis=-1)(encoder2);\n",
    "    encoder2 = Activation(\"relu\")(encoder2);\n",
    "    encoder2 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder2);\n",
    "    input_tensor = merge([input_tensor, encoder2], mode=\"sum\");\n",
    "    encoder3 = BatchNormalization(axis=-1)(input_tensor);\n",
    "    encoder3 = Activation(\"relu\")(encoder3);\n",
    "    encoder3 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder3);\n",
    "    encoder3 = BatchNormalization(axis=-1)(encoder3);\n",
    "    encoder3 = Activation(\"relu\")(encoder3);\n",
    "    encoder3 = Conv1D(filters=250, kernel_size=5, strides=1, padding=\"same\", dilation_rate=4)(encoder3);\n",
    "    encoder3 = BatchNormalization(axis=-1)(encoder3);\n",
    "    encoder3 = Activation(\"relu\")(encoder3);\n",
    "    encoder3 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder3);\n",
    "    input_tensor = merge([input_tensor, encoder3], mode=\"sum\");\n",
    "    encoder4 = BatchNormalization(axis=-1)(input_tensor);\n",
    "    encoder4 = Activation(\"relu\")(encoder4);\n",
    "    encoder4 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder4);\n",
    "    encoder4 = BatchNormalization(axis=-1)(encoder4);\n",
    "    encoder4 = Activation(\"relu\")(encoder4);\n",
    "    encoder4 = Conv1D(filters=250, kernel_size=5, strides=1, padding=\"same\", dilation_rate=8)(encoder4);\n",
    "    encoder4 = BatchNormalization(axis=-1)(encoder4);\n",
    "    encoder4 = Activation(\"relu\")(encoder4);\n",
    "    encoder4 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder4);\n",
    "    input_tensor = merge([input_tensor, encoder4], mode=\"sum\");\n",
    "    encoder5 = BatchNormalization(axis=-1)(input_tensor);\n",
    "    encoder5 = Activation(\"relu\")(encoder5);\n",
    "    encoder5 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder5);\n",
    "    encoder5 = BatchNormalization(axis=-1)(encoder5);\n",
    "    encoder5 = Activation(\"relu\")(encoder5);\n",
    "    encoder5 = Conv1D(filters=250, kernel_size=5, strides=1, padding=\"same\", dilation_rate=16)(encoder5);\n",
    "    encoder5 = BatchNormalization(axis=-1)(encoder5);\n",
    "    encoder5 = Activation(\"relu\")(encoder5);\n",
    "    encoder5 = Conv1D(filters=500, kernel_size=1, strides=1, padding=\"same\")(encoder5);\n",
    "    input_tensor = merge([input_tensor, encoder5], mode=\"sum\");\n",
    "    input_tensor = Activation(\"relu\")(input_tensor);\n",
    "    input_tensor = Conv1D(filters=500, kernel_size=1, padding=\"same\", activation=\"relu\")(input_tensor);\n",
    "    target_sequence = Input(shape=(maxlen,));\n",
    "    t0 = Input(shape=(1, 500));\n",
    "    target_input = Embedding(input_length=maxlen, input_dim=len(eng_index) + 1, output_dim=500)(target_sequence);\n",
    "    target_input = merge([t0, target_input], concat_axis=1, mode=\"concat\");\n",
    "    input_to_decoder_sequence = merge([input_tensor, target_input], concat_axis=-1, mode=\"concat\");\n",
    "    decoder1 = Conv1D(filters=1000, kernel_size=1, padding=\"same\")(input_to_decoder_sequence);\n",
    "    decoder1 = BatchNormalization(axis=-1)(decoder1);\n",
    "    decoder1 = Activation(\"relu\")(decoder1);\n",
    "    decoder1 = Conv1D(filters=500, kernel_size=3, padding=\"causal\", dilation_rate=1)(decoder1);\n",
    "    decoder1 = BatchNormalization(axis=-1)(decoder1);\n",
    "    decoder1 = Activation(\"relu\")(decoder1);\n",
    "    decoder1 = Conv1D(filters=1000, kernel_size=1, padding=\"same\")(decoder1);\n",
    "    output_tensor = merge([input_to_decoder_sequence, decoder1], mode=\"sum\");\n",
    "    decoder2 = BatchNormalization(axis=-1)(output_tensor);\n",
    "    decoder2 = Activation(\"relu\")(decoder2);\n",
    "    decoder2 = Conv1D(filters=1000, kernel_size=1, strides=1, padding=\"same\")(decoder2);\n",
    "    decoder2 = BatchNormalization(axis=-1)(decoder2);\n",
    "    decoder2 = Activation(\"relu\")(decoder2);\n",
    "    decoder2 = Conv1D(filters=500, kernel_size=3, padding=\"causal\", dilation_rate=2)(decoder2);\n",
    "    decoder2 = BatchNormalization(axis=-1)(decoder2);\n",
    "    decoder2 = Activation(\"relu\")(decoder2);\n",
    "    decoder2 = Conv1D(filters=1000, kernel_size=1, padding=\"same\")(decoder2);\n",
    "    output_tensor = merge([output_tensor, decoder2], mode=\"sum\");\n",
    "    decoder3 = BatchNormalization(axis=-1)(output_tensor);\n",
    "    decoder3 = Activation(\"relu\")(decoder3);\n",
    "    decoder3 = Conv1D(filters=1000, kernel_size=1, strides=1, padding=\"same\")(decoder3);\n",
    "    decoder3 = BatchNormalization(axis=-1)(decoder3);\n",
    "    decoder3 = Activation(\"relu\")(decoder3);\n",
    "    decoder3 = Conv1D(filters=500, kernel_size=3, padding=\"causal\", dilation_rate=4)(decoder3);\n",
    "    decoder3 = BatchNormalization(axis=-1)(decoder3);\n",
    "    decoder3 = Activation(\"relu\")(decoder3);\n",
    "    decoder3 = Conv1D(filters=1000, kernel_size=1, padding=\"same\")(decoder3);\n",
    "    output_tensor = merge([output_tensor, decoder3], mode=\"sum\");\n",
    "    decoder4 = BatchNormalization(axis=-1)(output_tensor);\n",
    "    decoder4 = Activation(\"relu\")(decoder4);\n",
    "    decoder4 = Conv1D(filters=1000, kernel_size=1, strides=1, padding=\"same\")(decoder4);\n",
    "    decoder4 = BatchNormalization(axis=-1)(decoder4);\n",
    "    decoder4 = Activation(\"relu\")(decoder4);\n",
    "    decoder4 = Conv1D(filters=500, kernel_size=3, padding=\"causal\", dilation_rate=8)(decoder4);\n",
    "    decoder4 = BatchNormalization(axis=-1)(decoder4);\n",
    "    decoder4 = Activation(\"relu\")(decoder4);\n",
    "    decoder4 = Conv1D(filters=1000, kernel_size=1, padding=\"same\")(decoder4);\n",
    "    output_tensor = merge([output_tensor, decoder4], mode=\"sum\");\n",
    "    decoder5 = BatchNormalization(axis=-1)(output_tensor);\n",
    "    decoder5 = Activation(\"relu\")(decoder5);\n",
    "    decoder5 = Conv1D(filters=1000, kernel_size=1, strides=1, padding=\"same\")(decoder5);\n",
    "    decoder5 = BatchNormalization(axis=-1)(decoder5);\n",
    "    decoder5 = Activation(\"relu\")(decoder5);\n",
    "    decoder5 = Conv1D(filters=500, kernel_size=3, padding=\"causal\", dilation_rate=16)(decoder5);\n",
    "    decoder5 = BatchNormalization(axis=-1)(decoder5);\n",
    "    decoder5 = Activation(\"relu\")(decoder5);\n",
    "    decoder5 = Conv1D(filters=1000, kernel_size=1, padding=\"same\")(decoder5);\n",
    "    output_tensor = merge([output_tensor, decoder5], mode=\"sum\");\n",
    "    output_tensor = Activation(\"relu\")(output_tensor);\n",
    "    # decoder=Dropout(0.1)(decoder);\n",
    "    result = Conv1D(filters=len(eng_index), kernel_size=1, padding=\"same\", activation=\"softmax\")(output_tensor);\n",
    "    model = Model(inputs=[input_sequence, target_sequence, t0], outputs=result);\n",
    "    opt = adam(lr=0.0003); # as in the paper, we choose adam optimizer with lr=0.0003\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['categorical_accuracy'],\n",
    "                  sample_weight_mode=\"temporal\");\n",
    "    return model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-05-21_22-51-59'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logger = CSVLogger('logs/'+now_str+ '.log')\n",
    "now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:95: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:105: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  app.launch_new_instance()\n",
      "/home/ivan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=235, workers=16, callbacks=[<keras.ca..., initial_epoch=0, epochs=1000)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "235/235 [==============================] - 169s 720ms/step - loss: 2.3139 - categorical_accuracy: 0.4509\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 163s 694ms/step - loss: 1.6330 - categorical_accuracy: 0.4769\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 163s 694ms/step - loss: 1.4540 - categorical_accuracy: 0.4758\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 164s 698ms/step - loss: 1.3400 - categorical_accuracy: 0.5219\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 161s 684ms/step - loss: 1.2455 - categorical_accuracy: 0.5283\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 159s 678ms/step - loss: 1.1594 - categorical_accuracy: 0.5267\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 158s 671ms/step - loss: 1.0772 - categorical_accuracy: 0.5231\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 157s 666ms/step - loss: 0.9887 - categorical_accuracy: 0.5021\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 157s 670ms/step - loss: 0.8990 - categorical_accuracy: 0.4927\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 158s 671ms/step - loss: 0.8039 - categorical_accuracy: 0.4890\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 158s 674ms/step - loss: 0.7100 - categorical_accuracy: 0.4884\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 159s 678ms/step - loss: 0.6166 - categorical_accuracy: 0.4964\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 160s 681ms/step - loss: 0.5209 - categorical_accuracy: 0.4971\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 159s 675ms/step - loss: 0.4452 - categorical_accuracy: 0.4983\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 159s 677ms/step - loss: 0.3740 - categorical_accuracy: 0.5107\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 158s 673ms/step - loss: 0.3195 - categorical_accuracy: 0.5100\n",
      "Epoch 17/1000\n",
      "172/235 [====================>.........] - ETA: 42s - loss: 0.2529 - categorical_accuracy: 0.5166"
     ]
    }
   ],
   "source": [
    "def train(batch_size,epochs,maxlen, N):\n",
    "    French,English=load_dataset(batch_size, data_path, N=N);\n",
    "    eng_index, french_index, index_eng, index_french=build_vacabulary(French,English);\n",
    "    model=build_model(french_index,eng_index,index_french,index_eng,English,French);\n",
    "    early = EarlyStopping(monitor=\"loss\", mode=\"min\", patience=5);\n",
    "    lr_change = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=0, min_lr=0.000)\n",
    "    checkpoint = ModelCheckpoint(filepath=WKDIR + \"/conv1d_\",\n",
    "                                 save_best_only=False, save_weights_only=False);# checkpoint the model after each epoch\n",
    "    \n",
    "    \n",
    "    \n",
    "    # start training !\n",
    "    history = model.fit_generator(generate_batch_data(English,French,eng_index,french_index,batch_size),\n",
    "                        steps_per_epoch=int(len(English) * batch_size ** -1),\n",
    "                        nb_epoch=epochs, workers=16, callbacks=[early, checkpoint, lr_change, logger ], \n",
    "                            initial_epoch=0);\n",
    "    model.save(WKDIR + \"/conv1d_rus_anc.h5\")# where the model is saved\n",
    "    \n",
    "    return history\n",
    "    \n",
    "N = 15000 #!!!\n",
    "batch_size = 64;\n",
    "maxlen = 205;\n",
    "epochs=1000\n",
    "history = train(batch_size,epochs,maxlen, N);# run baby run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
